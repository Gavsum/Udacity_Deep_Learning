{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project4 - Language Translation\n",
    "This project utilizes a sequence to sequence model on a data set of English and French sentences and will attempt to then translate new sentences from English to French once trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import \n",
    "The training data is composed only of a small portion of the respective languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en.txt'\n",
    "target_path = 'data/small_vocab_fr.txt'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few printouts to provide some insight into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "view_sentence_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Converting source text and target text datasets into numerical data input for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    source_id_text = [[source_vocab_to_int[x] for x in y.split()] for y in source_text.split('\\n')]\n",
    "    target_id_text = [[target_vocab_to_int[a] for a in b.split()] for b in target_text.split('\\n')]\n",
    "    \n",
    "    target_id_text = [entry + [target_vocab_to_int['<EOS>']]for entry in target_id_text]\n",
    "    \n",
    "    return source_id_text, target_id_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize helper to preprocess data and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from disc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Following function will create all the necessary placeholders for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    source_len = tf.placeholder(tf.int32, [None], name='source_sequence_length')\n",
    "    target_len = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_len)\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_len, max_target_len, source_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "Removes the last word id from each batch in target_data and concatinates the GO ID to the begining of each word batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # Drop last word id from each batch\n",
    "    trimmed_batch = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1, 1])\n",
    "\n",
    "    # Create GO_ID data to add to the batches\n",
    "    go_id = tf.fill([batch_size, 1], target_vocab_to_int[\"<GO>\"])\n",
    "    \n",
    "    # Consolidate Go ID & trimmed batch\n",
    "    processed = tf.concat([go_id, trimmed_batch], 1)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Embed encoding input and construct stacked LSTM cells with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,\n",
    "                  source_sequence_length, source_vocab_size, \n",
    "                  encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the embedding\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                            source_vocab_size,\n",
    "                                            encoding_embedding_size)\n",
    "    \n",
    "    # LSTM Cell stack\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    # Wrapping multi-cell stack w. dropout\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "    \n",
    "    # Create RNN output & state\n",
    "    rnn_out, rnn_state = tf.nn.dynamic_rnn(drop, embed, \n",
    "                                           sequence_length = source_sequence_length, \n",
    "                                           dtype = tf.float32)\n",
    "    \n",
    "    return rnn_out, rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Using the seq2seq functions in tf.contrib to obtain decoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input,\n",
    "                        target_sequence_length, max_summary_length,\n",
    "                        output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_sequence_length: The lengths of each sequence in the target batch\n",
    "    :param max_summary_length: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper for the training process\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = dec_embed_input,\n",
    "                                                       sequence_length = target_sequence_length,\n",
    "                                                       time_major = False)\n",
    "    \n",
    "    # Basic Decoder\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                                      training_helper,\n",
    "                                                      encoder_state,\n",
    "                                                      output_layer)\n",
    "    \n",
    "    # Perform dynamic encoding\n",
    "    training_output, state = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                              impute_finished = True,\n",
    "                                                              maximum_iterations = max_summary_length)\n",
    "    \n",
    "    return training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Again utilizing the tf seq2seq functions to create the inference decoder and obtain decoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
