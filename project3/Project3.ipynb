{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "This project will utilize RNN's to generate plausible TV scripts. Simpson's scripts from 27 seasons will be used as input. Specifically, those scenes that take place at Moe's Tavern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Number of unique words (Roughly): 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene\" 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring the data\n",
    "\n",
    "view_sentence_range = (0, 10)\n",
    "\n",
    "print(\"Dataset Stats\")\n",
    "print(\"Number of unique words (Roughly): {}\".format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print(\"Number of scenes: {}\".format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene\" {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print(\"Number of lines: {}\".format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print(\"Average number of words in each line: {}\".format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print(\"The sentences {} to {}:\".format(*view_sentence_range))\n",
    "print(\"\\n\".join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "To create a word embedding, first transform the words to ids. Following function will return a tuple containing two dictionaries: One to translate words to id's, and another to translate id's to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We need to split the script into a word array using spaces as delimiters. The punctuation used in the script would make it difficult for the network to differentiate between words like \"hello!\" and \"hello?\". The following function will tokenize the punctuation using a lookup table which will later be used to replace punctuation in the script so that they are interpreted individually instead of as a part of another word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    tokens = {\n",
    "        \".\"  : \"||Period||\",\n",
    "        \",\"  : \"||Comma||\",\n",
    "        \"\\\"\" : \"||Quotation_Mark||\",\n",
    "        \";\"  : \"||Semicolon||\",\n",
    "        \"!\"  : \"||Exclamation_Mark||\",\n",
    "        \"?\"  : \"||Question_Mark||\",\n",
    "        \"(\"  : \"||Left_Parentheses||\",\n",
    "        \")\"  : \"||Right_Parentheses||\",\n",
    "        \"--\" : \"||Dash||\",\n",
    "        \"\\n\" : \"||Return||\"\n",
    "    }\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data and save\n",
    "Utilizing the prepreocessing functions from the helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "The follownig code block will reload the preprocessed data so that all the above code does not need to be re-run each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "import tensorflow as tf\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Net Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "following function creates placeholders for the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return inputs, targets, learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell \n",
    "Stack tensorflow basicLSTM cells into a MultiRNNCell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    \n",
    "    lstm_layers = 3\n",
    "    \n",
    "    # TF 1.1 compatible -> basic lstm in multi rnn 1 line\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([\n",
    "                tf.contrib.rnn.BasicLSTMCell(rnn_size) for _ in range(lstm_layers)])\n",
    "    \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, 'initial_state')\n",
    "    \n",
    "    return cell, initial_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to input data using tensorflow. Returns the embedded sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                             vocab_size, \n",
    "                                             embed_dim)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "Utilize the rnn cells function to create the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    \n",
    "    return outputs, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the above functions to build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embedded = get_embed(input_data, \n",
    "                         vocab_size, \n",
    "                         embed_dim)\n",
    "    \n",
    "    out, final_state = build_rnn(cell, embedded)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(out, \n",
    "                                                vocab_size, \n",
    "                                                activation_fn=None)\n",
    "    \n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches\n",
    "\n",
    "Following function will create batches of input and targets using int_text. The batches will be Numpy arrays with the shape (num_of_batches, 2, batch_size, seq_length). Each batch contains two elements:\n",
    "\n",
    "* A single batch of input with shape [batch_size, sequence_length]\n",
    "* A single batch of tarets with shape [batch_Size, sequence_length]\n",
    "\n",
    "If the last batch cannont fill the batch size with enough data, it will be dropped. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "    inputs = np.array(int_text[:n_batches*batch_size*seq_length])\n",
    "    \n",
    "    # Shift elements by 1 to create targets\n",
    "    targets = np.array(int_text[1:n_batches*batch_size*seq_length+1])\n",
    "    targets[len(targets)-1] = inputs[0]\n",
    "    \n",
    "    # Reshape both\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    targets = targets.reshape((batch_size, -1))\n",
    "    \n",
    "    # Create Empty output shape\n",
    "    out_batches = np.zeros((n_batches, 2, batch_size, seq_length), dtype=int)\n",
    "    \n",
    "    # Fill output\n",
    "    ctr = 0\n",
    "    for n in range(0, inputs.shape[1], seq_length):\n",
    "        inpt_out = inputs[:,n:n+seq_length]\n",
    "        trgt_out = targets[:, n:n+seq_length]\n",
    "        \n",
    "        out_batches[ctr][0] = inpt_out\n",
    "        out_batches[ctr][1] = trgt_out\n",
    "        ctr += 1\n",
    "        \n",
    "    return out_batches\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 375\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 50\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, \n",
    "                                   rnn_size, \n",
    "                                   input_text,\n",
    "                                  vocab_size, \n",
    "                                   embed_dim)\n",
    "    \n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name=\"probs\")\n",
    "    \n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(logits, \n",
    "                                targets, \n",
    "                                tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gradients if grad is not None]\n",
    "    \n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "Training the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/13 train_loss = 8.822\n",
      "Epoch   3 Batch   11/13 train_loss = 6.144\n",
      "Epoch   7 Batch    9/13 train_loss = 6.126\n",
      "Epoch  11 Batch    7/13 train_loss = 6.165\n",
      "Epoch  15 Batch    5/13 train_loss = 6.030\n",
      "Epoch  19 Batch    3/13 train_loss = 6.080\n",
      "Epoch  23 Batch    1/13 train_loss = 6.039\n",
      "Epoch  26 Batch   12/13 train_loss = 6.074\n",
      "Epoch  30 Batch   10/13 train_loss = 6.038\n",
      "Epoch  34 Batch    8/13 train_loss = 5.880\n",
      "Epoch  38 Batch    6/13 train_loss = 5.919\n",
      "Epoch  42 Batch    4/13 train_loss = 5.339\n",
      "Epoch  46 Batch    2/13 train_loss = 4.830\n",
      "Epoch  50 Batch    0/13 train_loss = 4.539\n",
      "Epoch  53 Batch   11/13 train_loss = 4.370\n",
      "Epoch  57 Batch    9/13 train_loss = 4.247\n",
      "Epoch  61 Batch    7/13 train_loss = 4.100\n",
      "Epoch  65 Batch    5/13 train_loss = 3.814\n",
      "Epoch  69 Batch    3/13 train_loss = 3.818\n",
      "Epoch  73 Batch    1/13 train_loss = 3.617\n",
      "Epoch  76 Batch   12/13 train_loss = 3.481\n",
      "Epoch  80 Batch   10/13 train_loss = 3.519\n",
      "Epoch  84 Batch    8/13 train_loss = 3.239\n",
      "Epoch  88 Batch    6/13 train_loss = 3.029\n",
      "Epoch  92 Batch    4/13 train_loss = 2.900\n",
      "Epoch  96 Batch    2/13 train_loss = 2.866\n",
      "Epoch 100 Batch    0/13 train_loss = 2.562\n",
      "Epoch 103 Batch   11/13 train_loss = 2.442\n",
      "Epoch 107 Batch    9/13 train_loss = 2.586\n",
      "Epoch 111 Batch    7/13 train_loss = 2.388\n",
      "Epoch 115 Batch    5/13 train_loss = 2.230\n",
      "Epoch 119 Batch    3/13 train_loss = 2.311\n",
      "Epoch 123 Batch    1/13 train_loss = 1.900\n",
      "Epoch 126 Batch   12/13 train_loss = 1.734\n",
      "Epoch 130 Batch   10/13 train_loss = 1.696\n",
      "Epoch 134 Batch    8/13 train_loss = 1.617\n",
      "Epoch 138 Batch    6/13 train_loss = 1.552\n",
      "Epoch 142 Batch    4/13 train_loss = 1.401\n",
      "Epoch 146 Batch    2/13 train_loss = 1.440\n",
      "Epoch 150 Batch    0/13 train_loss = 1.303\n",
      "Epoch 153 Batch   11/13 train_loss = 1.195\n",
      "Epoch 157 Batch    9/13 train_loss = 1.096\n",
      "Epoch 161 Batch    7/13 train_loss = 0.986\n",
      "Epoch 165 Batch    5/13 train_loss = 0.935\n",
      "Epoch 169 Batch    3/13 train_loss = 0.988\n",
      "Epoch 173 Batch    1/13 train_loss = 1.061\n",
      "Epoch 176 Batch   12/13 train_loss = 1.030\n",
      "Epoch 180 Batch   10/13 train_loss = 1.025\n",
      "Epoch 184 Batch    8/13 train_loss = 0.902\n",
      "Epoch 188 Batch    6/13 train_loss = 0.881\n",
      "Epoch 192 Batch    4/13 train_loss = 0.890\n",
      "Epoch 196 Batch    2/13 train_loss = 0.783\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_i, (x,y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            \n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            \n",
    "            # Display infos every n batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print(\"Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}\".format(epoch_i,\n",
    "                                                                             batch_i,\n",
    "                                                                             len(batches),\n",
    "                                                                             train_loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print(\"Model Trained and Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Params\n",
    "Load model parameters for script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_t = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    init_state_t = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    final_state_t = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    probs_t = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    \n",
    "    return input_t, init_state_t, final_state_t, probs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    size = len(int_to_vocab)\n",
    "    the_word = np.random.choice(size, p=probabilities)\n",
    "    output = int_to_vocab[the_word]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "Following code block will use the model parameters previously trained to generate a new (semi) plausible Simpson's script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "homer_simpson: so not too guy? well, you little lease i ever got us\" bar no one american your brilliant man!\n",
      "homer_simpson:(sobs) oh thank you, you mr. this unsourced, homer. the greatest year of the thing you got their loser? get up my pin barkeep, the sturdy sittin' in the game!\n",
      "homer_simpson: i've had interested out of the game at my new game is a car, moe.\n",
      "crowd: yeah, you knockin' beer?\n",
      "barney_gumble: how happened to you? man, you can't do so people more.\n",
      "moe_szyslak: okay, i'm like some american comedy somethin'?\n",
      "fox_mulder: win this people were getting years.(to vicious) but i like to find salad house out of the bar.\n",
      "lenny_leonard: no way and just do i see the great year into a way to be just leaving, ladies up for my bar.\n",
      "barney_gumble: oh, i don't want anything to be fun.\n",
      "homer_simpson: i'm gonna get them to turn around out for all my favorite delighted down his face is\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'homer_simpson'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
